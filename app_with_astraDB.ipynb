{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16e1cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09408ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ba9d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe5c3a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa7769ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee11db7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_db_application_token = os.getenv(\"ASTRA_DB_APPLICATION_TOKEN\")\n",
    "astra_db_id = os.getenv(\"ASTRA_DB_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5f5e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LangChain_Projects\\Document QnA Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rajpu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "### Embeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad6aa81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfreader = PdfReader(\"Research Papers\\LLm.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc976cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "raw_text = \"\"\n",
    "for i, pages in enumerate(pdfreader.pages):\n",
    "    content = pages.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b864f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=astra_db_application_token, database_id=astra_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8116333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model = \"openai/gpt-oss-20b\", groq_api_key = os.getenv(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21709a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_stores = Cassandra(\n",
    "    embedding=embeddings,\n",
    "    table_name=\"qa_mini_db\",\n",
    "    session=None,\n",
    "    keyspace=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f8672da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size = 2000,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4321f899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8bf8ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 155 headlines.\n"
     ]
    }
   ],
   "source": [
    "astra_vector_stores.add_texts(texts=texts)\n",
    "\n",
    "print(\"Inserted %i headlines.\" %len(texts))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_stores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e17b3",
   "metadata": {},
   "source": [
    "# Run QA Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a1cc16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUESTION: \"What is self attention?\"\n",
      "ANSWER: \"**Self‑attention** is the core operation that lets a transformer model “look at” every other token in the same sequence when computing the representation for a particular token.  \n",
      "\n",
      "- **How it works**  \n",
      "  1. For each token in the input, the model computes three vectors: a **query** (Q), a **key** (K) and a **value** (V).  \n",
      "  2. The relevance of every other token to the current token is measured by the dot‑product of its query with all the keys:  \n",
      "     \\[\n",
      "     \\text{score}_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\n",
      "     \\]  \n",
      "     (the division by \\(\\sqrt{d_k}\\) stabilises gradients).  \n",
      "  3. The scores are passed through a softmax to obtain attention weights that sum to one.  \n",
      "  4. Each token’s new representation is the weighted sum of all values:  \n",
      "     \\[\n",
      "     \\text{output}_i = \\sum_j \\text{softmax}(\\text{score}_{ij}) \\, V_j\n",
      "     \\]  \n",
      "\n",
      "- **Why it’s called “self‑attention”**  \n",
      "  The queries, keys and values all come from the *same* block of tokens (the same encoder or decoder layer). That is, every token attends to every other token in the same sequence—hence “self‑attention”.\n",
      "\n",
      "- **What it gives the model**  \n",
      "  * **Long‑range dependencies**: tokens far apart can directly influence each other.  \n",
      "  * **Parallelism**: the dot‑product operations can be computed for all token pairs at once, making training efficient on modern GPUs/TPUs.  \n",
      "  * **Flexibility**: the same mechanism is reused in both encoder‑only (e.g., BERT) and decoder‑only (e.g., GPT) transformers, and it can be extended to cross‑attention in encoder‑decoder setups.\n",
      "\n",
      "In short, self‑attention lets every position in a sequence dynamically weigh the contributions of every other position, enabling powerful context‑aware representations in transformer‑based language models.\"\n",
      "\n",
      "FIRST DOCUMENTS BY RELEVANCE:\n",
      "    [0.7793] \"normalization of self-attention, CoRR abs /1910.05895 (2019). 24\n",
      "[299] Y . Liu, M. O ...\"\n",
      "    [0.7109] \"Attention in transformers [64] calculates query, key, and value\n",
      "mappings for input s ...\"\n",
      "    [0.7109] \"Attention in transformers [64] calculates query, key, and value\n",
      "mappings for input s ...\"\n",
      "    [0.7027] \"pre-training for language understanding and generation, arXiv preprint\n",
      "arXiv:2107.02 ...\"\n"
     ]
    }
   ],
   "source": [
    "first_question = True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text == \"\":\n",
    "        continue\n",
    "\n",
    "    first_question = False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_stores.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
